{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1suGewMzIKsC"
   },
   "source": [
    "# Lab 2\n",
    "\n",
    "The aims of the lab are to:\n",
    "\n",
    "*   Perform tokenization and stemming of text using NLTK\n",
    "*   Learn basics of cleaning ‘noisy’ text\n",
    "*   Calculate and visualize basic collection statistics of a corpus of text\n",
    "*   Practice using a bag-of-words using a one-hot encoding\n",
    "*   Learn to use SciKit learn to extract term frequency and TF-IDF features\n",
    "*   Use SciKit learn to ‘find similar’ documents using Cosine similarity\n",
    "*   Perform KMeans clustering on posts\n",
    "*   Learn to perform basic evaluation of clusters through manual inspection\n",
    "*   Use clustering to explore a corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "napJywoXLO7u"
   },
   "source": [
    "## Reddit Data Revisited\n",
    "\n",
    "The data is provided to download below. You can read more about the origins of the data at:\n",
    "https://github.com/google-research-datasets/coarse-discourse\n",
    "\n",
    "**Thread fields**\n",
    "*   URL - reddit URL of the thread\n",
    "*   title - title of the thread, as written by the first poster\n",
    "*   is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)\n",
    "*   subreddit - the subreddit of the thread\n",
    "*   posts - a list of all posts in the thread\n",
    "\n",
    "**Post fields**\n",
    "*   id - post ID, reddit ID of the current post\n",
    "*   in_reply_to - parent ID, reddit ID of the parent post, or the post that the current post is in reply to\n",
    "*   post_depth - the number of replies the current post is from the initial post\n",
    "*   is_first_post - True if the current post is the initial post\n",
    "*   annotations - a list of all annotations made to this post (see below)\n",
    "*   majority_type - the majority annotated type, if there is a majority type between the annotators, when considering only the main_type field\n",
    "*   majority_link - the majority annotated link, if there is a majority link between the annotators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw-TMlAZL7x2"
   },
   "source": [
    "Download the Reddit dataset\n",
    "<Insert reddit data description>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4527,
     "status": "ok",
     "timestamp": 1515845566288,
     "user": {
      "displayName": "Craig Macdonald",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116331980058935638219"
     },
     "user_tz": 0
    },
    "id": "OyYm3gDwJKQ-",
    "outputId": "51390c40-b244-443d-ea2d-ef7fb05b526a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 78.5M  100 78.5M    0     0  5125k      0  0:00:15  0:00:15 --:--:-- 5587k14 11.4M    0     0  5386k      0  0:00:14  0:00:02  0:00:12 5386k  4965k      0  0:00:16  0:00:14  0:00:02 3404k\n"
     ]
    }
   ],
   "source": [
    "# The local location to store the reddit dataset.\n",
    "local_file = \"coarse_discourse_dump_reddit.json\"\n",
    "\n",
    "# The ! performs a shell command to download the reddit dataset using curl (not on Windows).\n",
    "!curl -o  $local_file https://storage.googleapis.com/tad2018/coarse_discourse_dump_reddit.json\n",
    "\n",
    "#!gsutil cp gs://tad2018/coarse_discourse_dump_reddit.json /tmp/coarse_discourse_dump_reddit.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPyo3mtPvqux"
   },
   "source": [
    "Let's extract the post content out into a posts global data frame that we'll use for our processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "a-XGj_AFCMBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110595\n"
     ]
    }
   ],
   "source": [
    "# The reddit thread structure is nested with posts in a new content.\n",
    "# This block reads the file as json and creates a posts data frame.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# A temporary variable to store the list of posts.\n",
    "posts_tmp = list()\n",
    "\n",
    "with open(local_file) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    thread = json.loads(line)\n",
    "    for post in thread['posts']:\n",
    "      # Keep the thread title, subreddit, and url with each post.\n",
    "      posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
    "                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n",
    "print(len(posts_tmp))\n",
    "\n",
    "# Create the posts data frame with the right column labels.  \n",
    "labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n",
    "post_frame = pd.DataFrame(posts_tmp, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vtsmI785z32"
   },
   "source": [
    "## Tokenization and stemming with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pdb8u7RT0rDj"
   },
   "source": [
    "Let's perform some basic tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Mkk-DAI8O1l3"
   },
   "outputs": [],
   "source": [
    "# NLTK is the Natural Language Toolkit, and contains several language datasets\n",
    "# as well as implementations of many popular NLP algorithms.\n",
    "# HINT: You should look at what is available here when thinking about your coursework!\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# A simple tokenizer based on a regular expression; a series of whitespace chars.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_9WrpED3FDQ"
   },
   "source": [
    "We first create a basic 'normalization' or 'canonicalization' function that converts text into a standard format. The simple function performs tokenization and lowercasing. It uses the NLTK regular expression tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eRZLyxV23D7V"
   },
   "outputs": [],
   "source": [
    "def basic_canonicalize(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = tokenizer.tokenize(string)\n",
    "  for t in tokens:\n",
    "    # Normalizes the token by lowercasing it.\n",
    "    normalized_tokens.append(t.lower())\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLrpONOaDb6M"
   },
   "source": [
    "Now that we have our tokenizer, let's use it to start exploring the collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Vo-Ff6UHDdVM"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# This tokenizes the body posts and creates vector of tokens for each post.\n",
    "all_posts_tokenized = post_frame.body.apply(basic_canonicalize)\n",
    "\n",
    "# A single variable to hold the tokens across all posts.\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "\n",
    "# Put the entire array of tokens into a NLTK Frequency Distribution class.\n",
    "word_dist = nltk.FreqDist(all_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFW1L_BKHW_B"
   },
   "source": [
    "**Exercise**: \n",
    "\n",
    "*   How many posts are empty?\n",
    "*   What is the total number of words?\n",
    "*   What is the average length of the post?\n",
    "\n",
    "\n",
    "Use the API of NLTK's [FreqDist](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist) class to calculate the total number of words, the average length of a post. Using FreqDist for this may be overkill here, but it's a useful class for modeling language that we'll see next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fgxsuewdJNxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "for post in all_posts_tokenized:\n",
    "    if len(post)==0:\n",
    "        #print(post)\n",
    "        counter=counter+1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YCi7dM3FwfV"
   },
   "source": [
    "**Exercise**: Print out the 50 most frequent words in the collection, and their frequency using the FreqDist API. Hint: FreqDist extends [collections.Counter](https://docs.python.org/2/library/collections.html#collections.Counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WaMgJL4XJVZD",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 176166),\n",
       " ('i', 146882),\n",
       " ('to', 124121),\n",
       " ('a', 113949),\n",
       " ('and', 102499),\n",
       " ('it', 83866),\n",
       " ('you', 77818),\n",
       " ('of', 73506),\n",
       " ('that', 61859),\n",
       " ('is', 60014),\n",
       " ('in', 56321),\n",
       " ('for', 48310),\n",
       " ('s', 40905),\n",
       " ('t', 39430),\n",
       " ('on', 34514),\n",
       " ('but', 34365),\n",
       " ('with', 33206),\n",
       " ('have', 33180),\n",
       " ('be', 32589),\n",
       " ('this', 29944),\n",
       " ('my', 29500),\n",
       " ('if', 28520),\n",
       " ('are', 25468),\n",
       " ('not', 25461),\n",
       " ('as', 25383),\n",
       " ('can', 23862),\n",
       " ('or', 23414),\n",
       " ('was', 23357),\n",
       " ('so', 22401),\n",
       " ('they', 22291),\n",
       " ('just', 21495),\n",
       " ('your', 20866),\n",
       " ('like', 20160),\n",
       " ('at', 18739),\n",
       " ('what', 16815),\n",
       " ('do', 16595),\n",
       " ('would', 16367),\n",
       " ('there', 16150),\n",
       " ('m', 16050),\n",
       " ('me', 15910),\n",
       " ('get', 15712),\n",
       " ('all', 15610),\n",
       " ('he', 15354),\n",
       " ('from', 14850),\n",
       " ('one', 14605),\n",
       " ('out', 14295),\n",
       " ('about', 14281),\n",
       " ('will', 14073),\n",
       " ('up', 14037),\n",
       " ('don', 13540)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InSKMVLs3Skl"
   },
   "source": [
    "What do you see?  The most common words are functional words, often referred to as 'stopwords' because they contain little information.  We should clean the data by removing these 'noise' words that don't have significant meaning on their own.\n",
    "\n",
    "We need a 'real' tokenizer that does stopping, stemming, and filters out other 'noise' words. It just so happens that NLTK has a built in stopword list. Download it and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hOs4QgaeexI4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/motoofi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFx5O4wQA3oT"
   },
   "source": [
    "**Exercise:** Create a function, `canonicalize` that filters 'meaningless' words.\n",
    "\n",
    "Modify the function to filter out 'noise' words. Use the NLTK Porter stemmer to stem the text.\n",
    "*   What about 'stop' words?\n",
    "*   What about excessively short tokens or 'long' tokens?\n",
    "\n",
    "Hint: Does the order matter? What could happen if stemming is applied after stopping?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean8.02716631884584\n",
      "std:7.798074686968009\n",
      "max:1440\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokens = word_dist.keys()\n",
    "lengths = []\n",
    "for token in tokens:\n",
    "    lengths.append(len(token))\n",
    "std = np.std(lengths)\n",
    "mean = np.mean(lengths)\n",
    "print(\"mean{0}\".format(mean))\n",
    "print(\"std:{0}\".format(std))\n",
    "print(\"max:{0}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F08gmxJQwtdL"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the NLTK Porter Stemmer discussed in lecture to use.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Modify the code below to 1) not append noisy words: words in stop_words or words that are too long or too short.\n",
    "# Question: What are good values for too long or too short? Why?\n",
    "def canonicalize(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = tokenizer.tokenize(string)\n",
    "  for token in tokens:\n",
    "    # YOUR CODE HERE\n",
    "    # Ignore stop words and other 'noise' words (too short? too long?)\n",
    "    # Lowercase and stem the words.\n",
    "    # Does the order of stopping and stemming matter? \n",
    "    #if (token in stop_words) or ((mean-std)<len(token)<(mean+std)) :\n",
    "    normalized = token.lower()\n",
    "    stem_and_normilized = stemmer.stem(normalized)\n",
    "    if (stem_and_normilized in stop_words):\n",
    "        pass\n",
    "    elif (len(stem_and_normilized)<(mean+std)) and (mean-std<len(stem_and_normilized)):\n",
    "        normalized_tokens.append(stem_and_normilized)\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vl2IuLNcHpi-"
   },
   "source": [
    "Now, let's repeat our exercise.  How have the statistics changed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h_XrKqR6N-8g"
   },
   "outputs": [],
   "source": [
    "# This tokenizes the body posts and creates vector of tokens for each post.\n",
    "all_posts_tokenized = post_frame.body.apply(canonicalize)\n",
    "\n",
    "# A single variable to hold the tokens across all posts.\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "\n",
    "# Put the entire array of tokens into a NLTK Frequency Distribution class.\n",
    "word_dist = nltk.FreqDist(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYJB76cDK5iK"
   },
   "source": [
    "Use your code for most frequent words discussed above here to print out the most frequent 50 words again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZUUH6MNGLdbT",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 29957),\n",
       " ('wa', 23377),\n",
       " ('like', 22210),\n",
       " ('get', 20548),\n",
       " ('would', 16367),\n",
       " ('one', 15873),\n",
       " ('http', 15796),\n",
       " ('go', 14118),\n",
       " ('use', 13954),\n",
       " ('com', 13199),\n",
       " ('time', 12666),\n",
       " ('think', 12474),\n",
       " ('make', 11450),\n",
       " ('know', 10671),\n",
       " ('realli', 10655),\n",
       " ('game', 10593),\n",
       " ('want', 10370),\n",
       " ('good', 10328),\n",
       " ('ha', 9977),\n",
       " ('peopl', 9729),\n",
       " ('also', 9548),\n",
       " ('work', 9278),\n",
       " ('look', 9177),\n",
       " ('becaus', 8956),\n",
       " ('thing', 8920),\n",
       " ('onli', 8770),\n",
       " ('ani', 8708),\n",
       " ('tri', 8512),\n",
       " ('much', 8295),\n",
       " ('need', 8157),\n",
       " ('play', 7873),\n",
       " ('see', 7862),\n",
       " ('could', 7492),\n",
       " ('way', 7429),\n",
       " ('2', 7231),\n",
       " ('hi', 7191),\n",
       " ('even', 7170),\n",
       " ('well', 7140),\n",
       " ('1', 7097),\n",
       " ('thank', 6904),\n",
       " ('www', 6889),\n",
       " ('say', 6813),\n",
       " ('take', 6544),\n",
       " ('veri', 6269),\n",
       " ('still', 6142),\n",
       " ('lot', 6126),\n",
       " ('someth', 6019),\n",
       " ('year', 6014),\n",
       " ('3', 5919),\n",
       " ('start', 5785)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmJapSu9IbUO"
   },
   "source": [
    "Looking at this list, it still contains reddit-specific words that may not be meaningful. For example, 'http'? Update the stop word set and repeat the above block until you are happy with the list.\n",
    "\n",
    "**Exercise: ** Modify the stop_words set to include additional noise words. Re-run the posts tokenization and top word calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "P0B-Ag-_y9MS",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 29957),\n",
       " ('wa', 23377),\n",
       " ('like', 22210),\n",
       " ('get', 20548),\n",
       " ('would', 16367),\n",
       " ('one', 15873),\n",
       " ('go', 14118),\n",
       " ('use', 13954),\n",
       " ('com', 13199),\n",
       " ('time', 12666),\n",
       " ('think', 12474),\n",
       " ('make', 11450),\n",
       " ('know', 10671),\n",
       " ('realli', 10655),\n",
       " ('game', 10593),\n",
       " ('want', 10370),\n",
       " ('good', 10328),\n",
       " ('ha', 9977),\n",
       " ('peopl', 9729),\n",
       " ('also', 9548),\n",
       " ('work', 9278),\n",
       " ('look', 9177),\n",
       " ('becaus', 8956),\n",
       " ('thing', 8920),\n",
       " ('onli', 8770),\n",
       " ('ani', 8708),\n",
       " ('tri', 8512),\n",
       " ('much', 8295),\n",
       " ('need', 8157),\n",
       " ('play', 7873),\n",
       " ('see', 7862),\n",
       " ('could', 7492),\n",
       " ('way', 7429),\n",
       " ('2', 7231),\n",
       " ('hi', 7191),\n",
       " ('even', 7170),\n",
       " ('well', 7140),\n",
       " ('1', 7097),\n",
       " ('thank', 6904),\n",
       " ('www', 6889),\n",
       " ('say', 6813),\n",
       " ('take', 6544),\n",
       " ('veri', 6269),\n",
       " ('still', 6142),\n",
       " ('lot', 6126),\n",
       " ('someth', 6019),\n",
       " ('year', 6014),\n",
       " ('3', 5919),\n",
       " ('start', 5785),\n",
       " ('first', 5750)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.update(('http', ))\n",
    "all_posts_tokenized = post_frame.body.apply(canonicalize)\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "word_dist = nltk.FreqDist(all_tokens)\n",
    "word_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JBfGbpYBO8N"
   },
   "source": [
    "We will now look at how we represent words using a one-hot encoding in more detail.\n",
    "\n",
    "Look at the Vocabulary class below:\n",
    "- What does it do? \n",
    "- How are words mapped to numbers? \n",
    "- How are the words ordered? Why might we want to do this?\n",
    "- What is the 'unk' token? And when is it used?\n",
    "- What happens if you perform words_to_ids on a word not in the original tokens?\n",
    "- Why might you want special start and end tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O_F9De9u2YZ9"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  START_TOKEN = \"<s>\"\n",
    "  END_TOKEN = \"</s>\"\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    # Counter is a very useful built-in python collection for keeping counts, \n",
    "    # Instead of extending Counter like FreqDist, it's used as a member variable.\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    self.num_unigrams = sum(iter(self.unigram_counts.values()))\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
    "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency.\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in iter(self.id_to_word.items())}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience keep a set of unique words.\n",
    "    self.wordset = set(iter(self.word_to_id.keys()))\n",
    "\n",
    "    # Store special IDs.\n",
    "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def sentence_to_ids(self, words):\n",
    "    return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM2JtTPoI0O_"
   },
   "source": [
    "**Exercise:** Use the Vocabulary class to print out the top 10 most frequent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kRAQnbqiMpe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', 'thi', 'wa', 'like', 'get', 'would', 'one']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary(word_dist)\n",
    "x=vocab.ordered_words()\n",
    "x[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KQ3qqZEMqii"
   },
   "source": [
    "Below are some example uses of the Vocabulary object to map words to ids and vice-versa. Execute the code and make sure you understand what is happening with the Vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A1wNxrOEMrNC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[2]\n",
      "['<unk>']\n",
      "72326 ['x4m6']\n"
     ]
    }
   ],
   "source": [
    "# Use our vocabulary to map words to integer ids.\n",
    "# You should be able to predict the index of like based on your knowledge\n",
    "# the Vocabulary class and the most frequent words.\n",
    "print(vocab.words_to_ids([\"like\"]))\n",
    "\n",
    "# What index should this return?\n",
    "print(vocab.words_to_ids([\"likemymadeupword\"]))\n",
    "\n",
    "\n",
    "# What's special about 2? Can you predict what it will be?\n",
    "print(vocab.ids_to_words([2]))\n",
    "\n",
    "\n",
    "# And let's print out the words from the vocab.\n",
    "import random as rand\n",
    "\n",
    "# Pick a random token in the vocabulary\n",
    "idx = rand.randint(0, vocab.size-1)\n",
    "print(idx, vocab.ids_to_words([idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XWjf3yuTO6J"
   },
   "source": [
    "## Vector representations with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25XYnTCi2CWz"
   },
   "source": [
    "There are lots of ways to do the same thing in Python. Scikit-learn is a widely machine learning library that includes tools for performing operations on data: similarity computation, clustering, classification, and many others. We'll use Scikit-learn to create a vector representations of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q997lE7FTiyo"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Parallel arrays of the post keys and values.\n",
    "post_vals = list()\n",
    "post_keys = list()\n",
    "\n",
    "for index, post in post_frame.iterrows():\n",
    "    post_keys.append(post['id'])\n",
    "    post_vals.append(post['body'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qum0RqrykCDF"
   },
   "source": [
    "Create a simple TF term-document matrix with the TF counts using the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ETqhytXLkBDH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x82749 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 183 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We pass in our function tokenizer to the vectorizer object.\n",
    "#creating an CountVectorizer obj\n",
    "tf_vectorizer = CountVectorizer(tokenizer=canonicalize)\n",
    "\n",
    "# This creates a vocabulary like our Vocabulary object above.\n",
    "#create vocab for out corpera\n",
    "tf_vectorizer.fit(post_vals)\n",
    "\n",
    "# Now we create a sparse term-document matrix using the vocabulary.\n",
    "# This performs the mapping of tokens to their IDs.\n",
    "#make a representation for corpora\n",
    "tf_term_document_matrix = tf_vectorizer.transform(post_vals)\n",
    "\n",
    "\n",
    "# Note: These can be combined with fit_transform to do this in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqXwInGGOo2N"
   },
   "source": [
    "**Exercise**: Type in a string below. Look at it's vector representation. What happens to 'unk' words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4p0tn37ZW1Ji",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31368)\t2\n",
      "  (0, 63701)\t1\n",
      "  (0, 72092)\t1\n",
      "[array(['fine', 'salam', 'thank'], dtype='<U15')]\n"
     ]
    }
   ],
   "source": [
    "str = 'salam khobi? I am fine thanks. Mersi I am fine too.'\n",
    "response = tf_vectorizer.transform([str])\n",
    "print (response)\n",
    "print (tf_vectorizer.inverse_transform(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0Q0vgPe3dqD"
   },
   "source": [
    "The first step transforms our sentence using the vocbulary, creating a sparse TF vector representing the sentence.\n",
    "The second operation allows us to see what words correspond to each of those elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXmMp1CTWaWi"
   },
   "source": [
    "**Exercise:** Let's now upgrade from simple counts to a vectorizer that uses IDF. See the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) API.  The TF-IDF Vectorizer uses the raw term frequency by default, use the version of the constructor that uses the log(tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wh6T4S4mUcXR"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import TfidfVectorizer from sklearn and fill in the code below:\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=canonicalize ,sublinear_tf=True)\n",
    "tfidf_vectorizer.fit(post_vals)\n",
    "tfidf_term_document_matrix = tfidf_vectorizer.fit_transform(post_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1n8F135ZI6JO"
   },
   "source": [
    "TF-IDF vectorizer also includes support for creating n-grams during tokenization. Check the documentation for TfidfVectorizer and create an instance that includes word bigram tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jbr5xeEaI264"
   },
   "outputs": [],
   "source": [
    "# Limit the size of the vocabulary to the N most common words.\n",
    "num_features=500000\n",
    "\n",
    "ngram_vectorizer = TfidfVectorizer(max_features=num_features ,ngram_range=(2,2),sublinear_tf=True)\n",
    "ngram_vectorizer.fit(post_vals)\n",
    "ngram_term_document_matrix = ngram_vectorizer.fit_transform(post_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNJNCL46P3iI"
   },
   "source": [
    "Compare the n-gram representation of your string using the ngram vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XdGeP28VXdA3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 134752)\t0.42977123093042935\n",
      "  (0, 134742)\t0.4591820891496106\n",
      "  (0, 15936)\t0.7774628596072888\n",
      "[array(['fine too', 'fine thanks', 'am fine'], dtype='<U481')]\n"
     ]
    }
   ],
   "source": [
    "ngram_matrix = ngram_vectorizer.transform([str])\n",
    "print (ngram_matrix)\n",
    "print (ngram_vectorizer.inverse_transform(ngram_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5QjuYZn4pyx"
   },
   "source": [
    "We're just scratching the surface of what's possible with SKLearn's vectorizers.  There are other types of representations as well. For example, there is the [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) discussed in lecture as well others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEplJqQ4Zl4l"
   },
   "source": [
    "## Cosine similarity\n",
    "We will now use sklearn's cosine similarity implementation to find the most similar posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rWR6NZZIJKe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# A function that given an input query item returns the top-k most similar items \n",
    "# by their cosine similarity.\n",
    "def find_similar(query_vector, td_matrix, top_k = 5):\n",
    "    cosine_similarities = cosine_similarity(query_vector, td_matrix).flatten()\n",
    "    sorted_indices = cosine_similarities.argsort()[::-1]\n",
    "    return [(index, cosine_similarities[index]) for index in sorted_indices][0:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zc6zTizJLgs"
   },
   "source": [
    "**Exercise:** Next, lets pick a post to compute similarity with. Put the content of a post in the variable called `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PwE6Gu6YJXm0"
   },
   "outputs": [],
   "source": [
    "# Create an input 'query' string to find a similar post.\n",
    "query = \"machine learning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "smf4clmop-KY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:  (0, 204156)\t1.0\n",
      "\n",
      "Similar:\n",
      "0.30901671388901053 43311 https://www.reddit.com/r/compsci/comments/2qdyks/good_machine_learning_graduate_grad_labs/ Machine learning is distinct from AI, and both are distinct from robotics, as far as I'm aware, though of course there is overlap (many consider machine learning to be a branch of AI). Not really sure whether you're asking about ML, robotics, or ML applied to robotics?\n",
      "0.27281262895281994 82506 https://www.reddit.com/r/pebble/comments/3iokgu/why_do_i_have_to_send_usage_logs_to_pebble_just/ It's also easier to do the machine learning on the service side. Otherwise you're storing a TON of data locally on the device.\n",
      "0.23198046200964764 43300 https://www.reddit.com/r/compsci/comments/2qdyks/good_machine_learning_graduate_grad_labs/ I'm looking for recommendations for graduate programs and laboratories that have experience with machine learning and AI, particularly related to robotics or cyber physical applications. I have a mech.eng BS, significant mechanical experience, decent EE/CS/programming experience, I've been working in robotics R&D for the past two years, and I'm interested in moving more deliberately into machine learning. I like systems/project-management -level work, which is why I've resisted hyper-specializing and haven't done a graduate degree already, but machine learning is a sufficiently complicated and specific field that you can't teach yourself on weekends (at least not well enough that someone will hire you for it), so I'm looking for a few labs to talk to. (Obviously MIT, CMU, Stanford have great programs. I'm looking for specific professors/labs.)Recommendations? Thanks.\n",
      "0.2168695043037406 82507 https://www.reddit.com/r/pebble/comments/3iokgu/why_do_i_have_to_send_usage_logs_to_pebble_just/ Pebble also has a severely limited amount of space for storage. So it'd have to live in the phone app, making it bloated. It's better to have this data in a webservice, where it can be improved (via manual intervention or some sort of machine learning).\n",
      "0.1819160332378707 464 https://www.reddit.com/r/40something/comments/351n7m/hobbies/ I'm going to suggest a couple that I wish I was doing:* Woodworking* Geocaching* Taking a MOOC (I've had machine learning bookmarked for years)I second the fitness ideas thrown out.I guess I'd want to do something social, so outside of taking  woodworking class, none of mine are that social. So there's that. I hope you land on something that's rewarding. \n",
      "0.13736812863575806 85666 https://www.reddit.com/r/programming/comments/b5hne/askprogggit_as_programmers_how_often_do_you_use/ If you take probability (discrete math) you will have to integrate pdfs (if you can). In Machine Learning, you need to integrate to solve constraints (convex optimization). Personally I just scraped by in calculus and have regretted it, spending many hours relearning topics I could have already known.It depends on how serious you are about being an engineer and working on technically difficult (interesting) problems, however I recommend taking your higher maths seriously. It will help you many times in your career and serve as a valuable tool instead of a hindrance. \n",
      "0.0 36864 https://www.reddit.com/r/boardgames/comments/3425q7/wsig_bruges_concordia_or_both/ I agree, **Concordia** fits what you're looking for a little better. I quite like **Bruges** at 2-3p, but I don't think it scales up very well. Technically it's playable at 5p with the expansion, but I would rather not do that again.  In general, I like the expansion and it rejuvenated the game for me overall. **Concordia** is perfect at either 3p or 5p, though, and plays quite well at 2p and 4p even with less competition for resources and cities. There's an expansion map for variety, but it's not at all necessary. There is plenty of variety as it is in the tile setup and order that the cards come out.  It keeps getting better for me.  **Bruges** is an 8, but **Concordia** is a 10.\n",
      "0.0 36854 https://www.reddit.com/r/boardgames/comments/31qp5y/keezen/ Here is BBGs page [Keezbord](https://boardgamegeek.com/boardgame/93594/keezbord).  Looks like a Pachisi style game.  And not a very well know variant at that.\n",
      "0.0 36855 https://www.reddit.com/r/boardgames/comments/3425q7/wsig_bruges_concordia_or_both/ As tittle. I'm trying to figure out if I should get one over the other or just bite the bullet and get both. \n",
      "0.0 36856 https://www.reddit.com/r/boardgames/comments/3425q7/wsig_bruges_concordia_or_both/ Can you tell us a bit about yourself and your gaming group? How many players? Do you have a preference for game length maximum? How about letting us know what other games you liked/disliked and why?\n"
     ]
    }
   ],
   "source": [
    "# Temporary variables to easily change the vectorizer and the document_matrix.\n",
    "vectorizer = ngram_vectorizer\n",
    "document_matrix = ngram_term_document_matrix\n",
    "\n",
    "\n",
    "# Below are options to use an existing post content as a string query.\n",
    "#import random as rand\n",
    "#post_index = 1000\n",
    "#post_index = rand.randint(0, len(post_vals))\n",
    "#str = post_vals[post_index]\n",
    "#print(str.replace('\\n', ''))\n",
    "\n",
    "# Transform our string using the vocabulary.\n",
    "transformed = vectorizer.transform([query])\n",
    "query_transformed = transformed[0:1]\n",
    "print(\"query:{0}\".format(query_transformed))\n",
    "\n",
    "top_k = 10\n",
    "print (\"\\nSimilar:\")\n",
    "for index, score in find_similar(query_transformed, document_matrix, top_k):\n",
    "  print(score, index, post_keys[index], post_vals[index].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W66BwN2Z7dPc"
   },
   "source": [
    "What do you see?  If you use the post, it should find itself and return a similarity score of 1.0.\n",
    "\n",
    "Try changing the vectorizers and matrix representations to the ones you constructed (count, tfidf, or ngrams). \n",
    "How do the most similar posts change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIx1ORo4Ziga"
   },
   "source": [
    "## KMeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUhTG7OQ8Wag"
   },
   "source": [
    "What's in the reddit dataset? When we want to explore a dataset, one way to that is to cluster the data.\n",
    "\n",
    "As discussed in lecture, we'll now use an unsupervised clustering algorithm to cluster our posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj6xJlnz8BAN"
   },
   "source": [
    "\n",
    "From the SKlearn documentation:\n",
    "*The [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.*\n",
    "\n",
    "*K-means is often referred to as Lloyd’s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose k samples from the dataset X. After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yBFqE3-Mb2eE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 102935.694\n",
      "Iteration  1, inertia 102750.562\n",
      "Iteration  2, inertia 102715.011\n",
      "Iteration  3, inertia 102699.774\n",
      "Iteration  4, inertia 102693.206\n",
      "Iteration  5, inertia 102687.509\n",
      "Iteration  6, inertia 102684.441\n",
      "Iteration  7, inertia 102681.934\n",
      "Iteration  8, inertia 102678.603\n",
      "Iteration  9, inertia 102671.648\n",
      "Iteration 10, inertia 102663.218\n",
      "Iteration 11, inertia 102658.799\n",
      "Iteration 12, inertia 102657.232\n",
      "Iteration 13, inertia 102656.508\n",
      "Iteration 14, inertia 102655.504\n",
      "Iteration 15, inertia 102648.863\n",
      "Iteration 16, inertia 102635.023\n",
      "Iteration 17, inertia 102629.888\n",
      "Iteration 18, inertia 102629.105\n",
      "Iteration 19, inertia 102628.889\n",
      "Iteration 20, inertia 102628.757\n",
      "Iteration 21, inertia 102628.710\n",
      "Iteration 22, inertia 102628.677\n",
      "Iteration 23, inertia 102628.648\n",
      "Iteration 24, inertia 102628.614\n",
      "Iteration 25, inertia 102628.586\n",
      "Iteration 26, inertia 102628.529\n",
      "Iteration 27, inertia 102628.393\n",
      "Iteration 28, inertia 102628.311\n",
      "Iteration 29, inertia 102628.248\n",
      "Iteration 30, inertia 102628.153\n",
      "Iteration 31, inertia 102628.047\n",
      "Iteration 32, inertia 102627.965\n",
      "Iteration 33, inertia 102627.902\n",
      "Iteration 34, inertia 102627.877\n",
      "Iteration 35, inertia 102627.863\n",
      "Iteration 36, inertia 102627.845\n",
      "Iteration 37, inertia 102627.811\n",
      "Iteration 38, inertia 102627.793\n",
      "Iteration 39, inertia 102627.781\n",
      "Iteration 40, inertia 102627.775\n",
      "Iteration 41, inertia 102627.769\n",
      "Iteration 42, inertia 102627.767\n",
      "Converged at iteration 42: center shift 0.000000e+00 within tolerance 1.860053e-10\n",
      "Initialization complete\n",
      "Iteration  0, inertia 102932.188\n",
      "Iteration  1, inertia 102733.274\n",
      "Iteration  2, inertia 102721.635\n",
      "Iteration  3, inertia 102712.773\n",
      "Iteration  4, inertia 102704.853\n",
      "Iteration  5, inertia 102699.894\n",
      "Iteration  6, inertia 102698.484\n",
      "Iteration  7, inertia 102697.976\n",
      "Iteration  8, inertia 102697.869\n",
      "Iteration  9, inertia 102697.843\n",
      "Iteration 10, inertia 102697.840\n",
      "Iteration 11, inertia 102697.838\n",
      "Converged at iteration 11: center shift 0.000000e+00 within tolerance 1.860053e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=20, n_init=2, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 20\n",
    "\n",
    "# Note that there are multiple implementation of KMeans in sklearn. \n",
    "# The minibatchkmeans is faster, but less stable than the original kmeans.\n",
    "# It's faster, but produces different clusterings.\n",
    "#kmeans = MiniBatchKMeans(n_clusters=num_clusters, batch_size=500,  max_no_improvement=3)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init=2, verbose=10)\n",
    "kmeans.fit(document_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obt13q2h85su"
   },
   "source": [
    "We should now have a kmeans clustering with cluster centers, called 'centroids'. We'll print out the top 10 terms from each of the centroids. This will help us understand what each cluster represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GMZQiQtddm4s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " role in\n",
      " see the\n",
      " in this\n",
      " to see\n",
      " his shop\n",
      " his shoulders\n",
      " his shoulder\n",
      " his shot\n",
      " his short\n",
      " his shoes\n",
      "\n",
      "Cluster 1:\n",
      " opponents standing\n",
      " hurt your\n",
      " your opponents\n",
      " suggest that\n",
      " play with\n",
      " prestige ranking\n",
      " start competing\n",
      " well suggest\n",
      " regularly so\n",
      " with regularly\n",
      "\n",
      "Cluster 2:\n",
      " an aperture\n",
      " the lanterns\n",
      " the iso\n",
      " while keeping\n",
      " lets in\n",
      " more noise\n",
      " less light\n",
      " shutter speed\n",
      " then try\n",
      " but again\n",
      "\n",
      "Cluster 3:\n",
      " arcanist but\n",
      " the smn\n",
      " interesting ones\n",
      " the xpac\n",
      " smn questline\n",
      " because started\n",
      " flesh out\n",
      " be biased\n",
      " biased because\n",
      " more from\n",
      "\n",
      "Cluster 4:\n",
      " upvoted this\n",
      " because would\n",
      " advice but\n",
      " see some\n",
      " this because\n",
      " any advice\n",
      " would like\n",
      " have any\n",
      " don have\n",
      " like to\n",
      "\n",
      "Cluster 5:\n",
      " map stuff\n",
      " programmer but\n",
      " lead programmer\n",
      " enabled it\n",
      " to map\n",
      " here like\n",
      " stuff out\n",
      " think lot\n",
      " people around\n",
      " the lead\n",
      "\n",
      "Cluster 6:\n",
      " the intimidator\n",
      " say skip\n",
      " first defiant\n",
      " defiant predates\n",
      " love bob\n",
      " in 05\n",
      " 05 and\n",
      " bob and\n",
      " isn anywhere\n",
      " honesty the\n",
      "\n",
      "Cluster 7:\n",
      " valid vote\n",
      " howard much\n",
      " down julia\n",
      " accept valid\n",
      " people voted\n",
      " your attempts\n",
      " legitimate prime\n",
      " vote didn\n",
      " john howard\n",
      " cannot accept\n",
      "\n",
      "Cluster 8:\n",
      " hell was\n",
      " wondering how\n",
      " was there\n",
      " the hell\n",
      " anyone else\n",
      " this game\n",
      " how the\n",
      " in this\n",
      " his shoes\n",
      " his shop\n",
      "\n",
      "Cluster 9:\n",
      " check that\n",
      " you going\n",
      " now is\n",
      " or are\n",
      " to check\n",
      " are you\n",
      " is the\n",
      " going to\n",
      " his shop\n",
      " his shot\n",
      "\n",
      "Cluster 10:\n",
      " alright thanks\n",
      " the help\n",
      " thanks for\n",
      " for the\n",
      " thanks man\n",
      " thanks ton\n",
      " one thanks\n",
      " cool thanks\n",
      " the clarification\n",
      " oh my\n",
      "\n",
      "Cluster 11:\n",
      " that desperate\n",
      " this java\n",
      " that df\n",
      " client if\n",
      " faster too\n",
      " cpu core\n",
      " trick and\n",
      " for client\n",
      " whole computer\n",
      " an otherwise\n",
      "\n",
      "Cluster 12:\n",
      " of the\n",
      " in the\n",
      " if you\n",
      " for the\n",
      " to be\n",
      " on the\n",
      " you can\n",
      " you re\n",
      " to the\n",
      " thank you\n",
      "\n",
      "Cluster 13:\n",
      " not funny\n",
      " it not\n",
      " be nice\n",
      " also it\n",
      " funny anymore\n",
      " explain it\n",
      " to explain\n",
      " it it\n",
      " you have\n",
      " have to\n",
      "\n",
      "Cluster 14:\n",
      " and midgame\n",
      " her dash\n",
      " but late\n",
      " and loses\n",
      " on ton\n",
      " other adcs\n",
      " her auto\n",
      " she falls\n",
      " strong early\n",
      " hard just\n",
      "\n",
      "Cluster 15:\n",
      " roald dahl\n",
      " picture book\n",
      " than reading\n",
      " of picture\n",
      " reading book\n",
      " nope it\n",
      " was more\n",
      " you mean\n",
      " more of\n",
      " it was\n",
      "\n",
      "Cluster 16:\n",
      " over whelemed\n",
      " services provider\n",
      " software packages\n",
      " rapidly growing\n",
      " packages for\n",
      " testing these\n",
      " managed services\n",
      " been testing\n",
      " provider that\n",
      " was lacking\n",
      "\n",
      "Cluster 17:\n",
      " wikipedia org\n",
      " en wikipedia\n",
      " org wiki\n",
      " http en\n",
      " https en\n",
      " http www\n",
      " www np\n",
      " np reddit\n",
      " com autowikibot\n",
      " to autowikibot\n",
      "\n",
      "Cluster 18:\n",
      " youtube com\n",
      " www youtube\n",
      " com watch\n",
      " http www\n",
      " https www\n",
      " reddit com\n",
      " www reddit\n",
      " conspiracy comments\n",
      " com conspiracy\n",
      " link http\n",
      "\n",
      "Cluster 19:\n",
      " do you\n",
      " what do\n",
      " you have\n",
      " how do\n",
      " you think\n",
      " you mean\n",
      " you know\n",
      " why do\n",
      " you want\n",
      " have any\n",
      "\n"
     ]
    }
   ],
   "source": [
    "centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "tokens = vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "  print(\"Cluster %d:\" % i)\n",
    "  for ind in centroids[i, :10]:\n",
    "    print(' %s' % tokens[ind])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8f0AE1Y9_-G"
   },
   "source": [
    "Could you put a label on the clusters? Do the cluster terms look meaningful?\n",
    "\n",
    "\n",
    "Let's explore the clustering in more detail by looking at the assignments of posts to the clusters. We'll print out the contents by looking a sample of the top 10 posts per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aTkj56Tzklk3"
   },
   "outputs": [],
   "source": [
    "# Group the posts by their cluster labels.\n",
    "clustering = collections.defaultdict(list)\n",
    "for idx, label in enumerate(kmeans.labels_):\n",
    "  clustering[label].append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rcwzuz7vlvS5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster: 12  Num posts:  107887\n",
      "0 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ 4/7/13  7/27/12  http://www.imdb.com/title/tt0073440/referenceIt was only a..\n",
      "1 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ I've wanted to watch this for a long time. I was also turned off by the cou..\n",
      "2 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ You strike me as the type who would appreciate it. I would give it a go. Th..\n",
      "3 https://www.reddit.com/r/100movies365days/comments/1bx6qw/dtx120_87_nashville/ Yeah, I've always heard that Altman was famous for his ensemble casts. But ..\n",
      "4 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ **January 16th 3 Sets:** Went out shopping with my grandma as I visited her..\n",
      "5 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ grandmas are the best wingmen.\n",
      "6 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ dude, these sets are awesome. You're doing great. Sounds like you're a natu..\n",
      "7 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ Thanks man! Yeah I'm trying to just keep the \"who cares have fun\" attitude ..\n",
      "8 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ Ok. Update! Sorry I haven't been doing this day to day like I should be. Le..\n",
      "9 https://www.reddit.com/r/100sets/comments/omv7p/male_23_years_old_going_for_100_sets/ More updates!\n",
      "10 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ I love cheese cake! I love both making and eating it, so I'm sad to see tha..\n",
      "\n",
      "Cluster: 17  Num posts:  275\n",
      "15 https://www.reddit.com/r/1200isplenty/comments/259tbh/122cal_black_currant_cheesecake/ #####&#009;######&#009;####&#009; [**Digestive biscuit**](https://en.wikipe..\n",
      "1305 https://www.reddit.com/r/Android/comments/vzlrq/i_want_to_go_no_contract_whats_the_best_way_to_do/ Straight Talk which allows you to choose if you want to use AT&T or T-Mobil..\n",
      "1842 https://www.reddit.com/r/AskEurope/comments/4jlxfq/if_your_country_has_a_3035_hour_work_week_what_do/ Learn an instrument or a language, cooking, cycling, golf, cocaine, go to c..\n",
      "1869 https://www.reddit.com/r/AskEurope/comments/4jlxfq/if_your_country_has_a_3035_hour_work_week_what_do/ [Vereine](https://en.wikipedia.org/wiki/Voluntary_association) mostly\n",
      "2149 https://www.reddit.com/r/AskReddit/comments/10cteb/i_need_to_create_a_wikipedia_entry_how_do_i_stop/ Well prepare to be fired then. Wikipedia is very strict about new entries a..\n",
      "2173 https://www.reddit.com/r/AskReddit/comments/113tht/dear_reddit_what_does_the_date_may_18_1967_mean/ [The day after Tennessee repealed the Butler Act, a.k.a. the Monkey Law](ht..\n",
      "2183 https://www.reddit.com/r/AskReddit/comments/113tht/dear_reddit_what_does_the_date_may_18_1967_mean/ Here's [events that happened on the 18th of May.](http://en.wikipedia.org/w..\n",
      "2250 https://www.reddit.com/r/AskReddit/comments/12wms6/south_park_had_a_postelection_episode_air_on/ They actually [produce those things fairly quickly](http://en.wikipedia.org..\n",
      "2526 https://www.reddit.com/r/AskReddit/comments/1at5ow/what_book_do_you_think_is_beautifully_written/ [Red herring fallacy]( http://en.m.wikipedia.org/wiki/Red_herring)Again:>so..\n",
      "2737 https://www.reddit.com/r/AskReddit/comments/1eyg4h/if_the_president_of_the_united_states_promised_a/ ITERhttp://en.wikipedia.org/wiki/ITERWe need new energy sources and this is..\n",
      "3338 https://www.reddit.com/r/AskReddit/comments/1xix6t/serious_is_there_a_word_for_someone_or_the_action/ You may be thinking of [Ad hominem](http://en.wikipedia.org/wiki/Ad_hominem..\n",
      "\n",
      "Cluster: 19  Num posts:  1405\n",
      "20 https://www.reddit.com/r/1911/comments/16h61h/need_help_finding_a_springfield/ Where do you live? Eagle Guns and Range in Concord, NC has one in stock. Wa..\n",
      "59 https://www.reddit.com/r/2007scape/comments/1uz5j7/training_or_slayer/ That depends, do you want to train slayer? If you do, just train it instead..\n",
      "145 https://www.reddit.com/r/2007scape/comments/335w7l/gathering_at_druidic_circle_tomorrow_at_420pm/ What quest do you need to have done?\n",
      "309 https://www.reddit.com/r/2k14OA/comments/1tc3xq/1220_conference_semifinalsfinals_game_thread/ Wolves, do you think you'll be available around 8 PST?\n",
      "360 https://www.reddit.com/r/3Dprinting/comments/1y6k48/what_can_i_do_to_get_a_better_print/ Hi all, I just got my own desktop 3d printer. Yay! I printed a tiny cuddlin..\n",
      "372 https://www.reddit.com/r/3Dprinting/comments/3hge5f/best_remeshingrepair_software/ I've used netfabb before but it doesn't always keep the detail you want and..\n",
      "405 https://www.reddit.com/r/3d6/comments/41cgzk/dd5eneed_help_making_a_halfling_barbarian/ Why do you get to pick one of the three die you rolled?You can reroll the l..\n",
      "640 https://www.reddit.com/r/AMA/comments/4ee3q7/im_chrisacrosstheworld_japans_first_salaried/ What are your dreams and aspiration.What do you think is the perfect way to..\n",
      "656 https://www.reddit.com/r/AMA/comments/4ee3q7/im_chrisacrosstheworld_japans_first_salaried/ Thanks!! Do you want to leave? If you could go anywhere, where would you wa..\n",
      "664 https://www.reddit.com/r/AMA/comments/4ee3q7/im_chrisacrosstheworld_japans_first_salaried/ Serious question, that salary (180k/month) is really low, how else do you s..\n",
      "695 https://www.reddit.com/r/ASU/comments/2xj6lw/does_asu_have_a_sound_room_to_record_things_in/ What do you mean if you dont mind me asking? I contacted Mr. Hackbarth in T..\n",
      "\n",
      "Cluster: 18  Num posts:  987\n",
      "48 https://www.reddit.com/r/2007scape/comments/195abs/good_youtubers_to_watch_for_2007/ This guy is great, he does quests.  http://www.youtube.com/user/ThirdAgeFil..\n",
      "469 https://www.reddit.com/r/45thworldproblems/comments/1302wb/i_thought_i_was_but_then_i_didnt_now_i_dont_know/ [25](http://www.reddit.com/r/45thworldproblems/comments/1320td/date_2b1r1a_..\n",
      "470 https://www.reddit.com/r/45thworldproblems/comments/1302wb/i_thought_i_was_but_then_i_didnt_now_i_dont_know/ [Observe](http://www.reddit.com/r/45thworldproblems/comments/1301yn/date_2r..\n",
      "490 https://www.reddit.com/r/49ers/comments/40zhxo/im_finally_ready_to_give_up_my_harbaugh_flair_for/ https://www.reddit.com/r/ChipKellygifs\n",
      "642 https://www.reddit.com/r/AMA/comments/4ee3q7/im_chrisacrosstheworld_japans_first_salaried/ > Butter.[you sure](https://www.youtube.com/watch?v=TcHUDQoMId4)\n",
      "1135 https://www.reddit.com/r/Anarcho_Capitalism/comments/2sxs39/im_looking_for_an_essay_that_argues_how_the_us/ One name for your essay: [Robbert Higgs](http://mises.org/profile/robert-hi..\n",
      "1225 https://www.reddit.com/r/Android/comments/d1rua/canadians_you_can_buy_an_htc_desire_same_hardware/ It's well documented, but depending on how you use it you may never actuall..\n",
      "1323 https://www.reddit.com/r/Android/comments/vzlrq/i_want_to_go_no_contract_whats_the_best_way_to_do/ http://www.reddit.com/r/Android/comments/uytcv/fyi_list_of_the_us_prepaid_p..\n",
      "1808 https://www.reddit.com/r/AskEngineers/comments/128tqk/hey_askengineers_can_you_give_me_some_hope_what/ I am currently working on integrating fluid dynamics and structural dynamic..\n",
      "2134 https://www.reddit.com/r/AskMen/comments/4gxqwl/how_should_youre_prettyhot_for_a_comments_be/ https://www.youtube.com/watch?v=4jLT7GQYNhIYou'll need to see the entire th..\n",
      "2291 https://www.reddit.com/r/AskReddit/comments/14dtff/what_pop_culture_expression_or_any_phrase_really/ [Probably this] (http://www.youtube.com/watch?v=HcXQK_sWtMY)\n",
      "\n",
      "Cluster: 10  Num posts:  22\n",
      "173 https://www.reddit.com/r/2007scape/comments/3cle3y/old_account/ Thanks for the help!\n",
      "1114 https://www.reddit.com/r/Amsterdam/comments/4d63kb/potential_scam_asking_for_a_scanned_passport/ Thanks for the help!\n",
      "1926 https://www.reddit.com/r/AskHistorians/comments/2ns3zy/was_hannibal_black/ Alright, thanks, so I just misunderstood.\n",
      "9678 https://www.reddit.com/r/Christianity/comments/19kaws/not_a_christian_but_have_a_question/ Alright, thanks for the help! Just trying to understand.\n",
      "9938 https://www.reddit.com/r/Civcraft/comments/1lj46y/prussian_prisoner_report_pavel_the_hitman/ Alright, thanks for the clarification.\n",
      "14885 https://www.reddit.com/r/GTAV/comments/3yi24q/need_help_spending_money_wisely_gtav_online/ alright thanks.\n",
      "15808 https://www.reddit.com/r/Guildwars2/comments/3clz89/can_i_run_guild_wars_2_on_a_lenovo_u310/ the 1.8ghz one thanks for the help!\n",
      "16092 https://www.reddit.com/r/Guitar/comments/n2in9/hey_rguitar_im_auditioning_for_a_music_school_in/ Alright Thanks!\n",
      "23509 https://www.reddit.com/r/RocketLeague/comments/3crycf/is_it_a_huge_disadvantage_to_use_mousekeyboard/ Alright thanks for the answer!\n",
      "25725 https://www.reddit.com/r/SuggestALaptop/comments/26uyuz/want_something_that_wont_break_1415_usa_300500/ Cool, thanks for the help!\n",
      "32008 https://www.reddit.com/r/asktransgender/comments/2km20a/ftm_good_exercises_to_reduce_breast_size/ alright, thanks c:\n",
      "\n",
      "Cluster: 13  Num posts:  4\n",
      "12986 https://www.reddit.com/r/Drugs/comments/1g2bki/parents_of_rdrugs_how_do_you_responsibly_use_has/ Not funny\n",
      "85895 https://www.reddit.com/r/puns/comments/1zts00/a_friend_and_i_were_talking_about_a_book/ If you have to explain it, it's not funny anymore.\n",
      "87064 https://www.reddit.com/r/reddit.com/comments/8jrjs/if_all_the_stupid_people_have_a_lot_of_kids_and/ Also, it's not funny. Sadly...\n",
      "105678 https://www.reddit.com/r/vermont/comments/2intqd/planning_a_weekend_trip_nicest_towns_to_visit_in/ Not funny.  Be nice.\n",
      "\n",
      "Cluster: 1  Num posts:  1\n",
      "20777 https://www.reddit.com/r/Netrunner/comments/4452ws/store_championship_tourny_opinion/ If you have not actually played before I strongly suggest that you go but n..\n",
      "\n",
      "Cluster: 9  Num posts:  1\n",
      "27613 https://www.reddit.com/r/WaltDisneyWorld/comments/2npk4x/we_leave_for_the_airport_in_12_hours/ Now is the swiffer a carryon or are you going to check that?\n",
      "\n",
      "Cluster: 7  Num posts:  2\n",
      "33475 https://www.reddit.com/r/australia/comments/1ker4s/what_happens_if_you_place_an_independent_party_as/ It's strange how you cannot accept a valid vote.  I didn't like John Howard..\n",
      "33476 https://www.reddit.com/r/australia/comments/1ker4s/what_happens_if_you_place_an_independent_party_as/ >It's strange how you cannot accept a valid vote. I didn't like John Howard..\n",
      "\n",
      "Cluster: 16  Num posts:  1\n",
      "43356 https://www.reddit.com/r/computertechs/comments/1370qk/which_psa_do_you_or_your_company_use_we_are/ We have been testing these types of software packages for over a year.  We ..\n",
      "\n",
      "Cluster: 11  Num posts:  1\n",
      "48738 https://www.reddit.com/r/dwarffortress/comments/h5kd5/dfmc_crosspost/ Why would you want to use Minecraft of all things for a client?If you're re..\n",
      "\n",
      "Cluster: 3  Num posts:  1\n",
      "54468 https://www.reddit.com/r/ffxiv/comments/31e87k/do_the_12_really_have_any_importance_in_the_story/ I hope they flesh out more from the SMN questline in the xpac! I might be b..\n",
      "\n",
      "Cluster: 8  Num posts:  1\n",
      "61802 https://www.reddit.com/r/hearthstone/comments/32s3yt/the_ultimate_emperor_thaurissan_value/ Anyone else wondering how the hell was there a cruel taskmaster in this gam..\n",
      "\n",
      "Cluster: 14  Num posts:  1\n",
      "68915 https://www.reddit.com/r/leagueoflegends/comments/2mvxaq/kalista_to_nerf_or_not_to_nerf_post_your_thoughts/ I think that her dash makes her pretty strong early and midgame but late ga..\n",
      "\n",
      "Cluster: 4  Num posts:  1\n",
      "70532 https://www.reddit.com/r/legaladvice/comments/1l58j4/oh_my_dad_is_being_accused_of_some_serious_stuff/ I don't have any advice but I upvoted this because I would like to see some..\n",
      "\n",
      "Cluster: 0  Num posts:  1\n",
      "74099 https://www.reddit.com/r/masseffect/comments/sgpeo/st_mass_effect_cont/ i would likee to see the councils role in this.\n",
      "\n",
      "Cluster: 5  Num posts:  1\n",
      "77931 https://www.reddit.com/r/networking/comments/33kwsi/the_dude_configuration_settings/ I use observium... I don't think a lot of people around here like the lead ..\n",
      "\n",
      "Cluster: 6  Num posts:  1\n",
      "80684 https://www.reddit.com/r/paintball/comments/18n50a/bob_long_defiant_2_maybe/ As much as I love bob and his markers, I say skip the defiant. That marker ..\n",
      "\n",
      "Cluster: 2  Num posts:  1\n",
      "83652 https://www.reddit.com/r/photography/comments/1f13ac/photographing_flying_lanternsglowing_balloons/ Depending on how fast the lanterns are moving, that will determine the spee..\n",
      "\n",
      "Cluster: 15  Num posts:  1\n",
      "101607 https://www.reddit.com/r/tipofmytongue/comments/16l9ym/tomt_kids_hardback_story_book_about_a_girl_i/ If you mean Roald Dahl's, nope. It was more of a picture book than a readin..\n"
     ]
    }
   ],
   "source": [
    "for cluster, indices in clustering.items():\n",
    "  print(\"\\nCluster:\", cluster, \" Num posts: \", len(indices))\n",
    "  cur_docs = 0\n",
    "  for index in indices:\n",
    "    if (cur_docs > 10):\n",
    "      break\n",
    "    post_contents = post_vals[index].replace('\\n', '')\n",
    "    print(index, post_keys[index], (post_contents[:75] + '..') if len(post_contents) > 75 else post_contents)\n",
    "    cur_docs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKLaVZHMAF52"
   },
   "source": [
    "\n",
    "\n",
    "*   Is the clustering useful to explore the data?\n",
    "*   Can you label the clusters now that you've seen some examples?\n",
    "*   Are these 'good' clusters?\n",
    "\n",
    "If you have time, try changing the number of clusters - e.g 50 or 100. What do you observe about differences?\n",
    "\n",
    "Note: KMeans can be somewhat slow to execute exactly. See the MiniBatchKmeans variable above.  If you have time, try clustering using this clustering algorithm.  Do you see any significant differences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW03I8Kn0KCq"
   },
   "source": [
    "## Summary \n",
    "\n",
    "\n",
    "*   We've explored one-hot vector representations of text using several libraries.\n",
    "*   We've created count, tf-idf, and n-gram representations of text vectors.\n",
    "*   We've used the vectors to perform unsupervised clustering using scikit learn.\n",
    "*   Clustered the posts using KMeans\n",
    "\n",
    "\n",
    "When and why is clustering useful?\n",
    "*   Clusters can be used as features in a task (text classification and others)\n",
    "*   Used to perform exploratory data analysis (What's in there?)\n",
    "*   A fast, easy baseline that scales well and doesn't required labeled data\n",
    "\n",
    "Next time we'll experiment with additional language prediction tasks using the Vocabulary data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riYInv1ZShyU"
   },
   "source": [
    "## Please complete the Moodle feedback quiz for this lab\n",
    "\n",
    "[Quiz for Lab 2](http://moodle2.gla.ac.uk/mod/feedback/view.php?id=827639)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Lab 2.ipynb",
   "provenance": [
    {
     "file_id": "1zKMToiljPWRD8lp69HATAhkRj-OqefOV",
     "timestamp": 1515772404172
    },
    {
     "file_id": "1hNjXtYEZMlzFe3yJXN-4orZdmfX7QW9B",
     "timestamp": 1515088413611
    },
    {
     "file_id": "1Y80cJDxLkDZtvTyRcbPy8xMPAbyNmVXE",
     "timestamp": 1513709680152
    },
    {
     "file_id": "1fARiTtWW0pN80eg7XqEhbUbEtelPf5HT",
     "timestamp": 1513184264238
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
