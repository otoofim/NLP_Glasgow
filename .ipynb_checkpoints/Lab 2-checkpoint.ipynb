{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1suGewMzIKsC"
   },
   "source": [
    "# Lab 2\n",
    "\n",
    "The aims of the lab are to:\n",
    "\n",
    "*   Perform tokenization and stemming of text using NLTK\n",
    "*   Learn basics of cleaning ‘noisy’ text\n",
    "*   Calculate and visualize basic collection statistics of a corpus of text\n",
    "*   Practice using a bag-of-words using a one-hot encoding\n",
    "*   Learn to use SciKit learn to extract term frequency and TF-IDF features\n",
    "*   Use SciKit learn to ‘find similar’ documents using Cosine similarity\n",
    "*   Perform KMeans clustering on posts\n",
    "*   Learn to perform basic evaluation of clusters through manual inspection\n",
    "*   Use clustering to explore a corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "napJywoXLO7u"
   },
   "source": [
    "## Reddit Data Revisited\n",
    "\n",
    "The data is provided to download below. You can read more about the origins of the data at:\n",
    "https://github.com/google-research-datasets/coarse-discourse\n",
    "\n",
    "**Thread fields**\n",
    "*   URL - reddit URL of the thread\n",
    "*   title - title of the thread, as written by the first poster\n",
    "*   is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link)\n",
    "*   subreddit - the subreddit of the thread\n",
    "*   posts - a list of all posts in the thread\n",
    "\n",
    "**Post fields**\n",
    "*   id - post ID, reddit ID of the current post\n",
    "*   in_reply_to - parent ID, reddit ID of the parent post, or the post that the current post is in reply to\n",
    "*   post_depth - the number of replies the current post is from the initial post\n",
    "*   is_first_post - True if the current post is the initial post\n",
    "*   annotations - a list of all annotations made to this post (see below)\n",
    "*   majority_type - the majority annotated type, if there is a majority type between the annotators, when considering only the main_type field\n",
    "*   majority_link - the majority annotated link, if there is a majority link between the annotators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hw-TMlAZL7x2"
   },
   "source": [
    "Download the Reddit dataset\n",
    "<Insert reddit data description>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 4
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4527,
     "status": "ok",
     "timestamp": 1515845566288,
     "user": {
      "displayName": "Craig Macdonald",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116331980058935638219"
     },
     "user_tz": 0
    },
    "id": "OyYm3gDwJKQ-",
    "outputId": "51390c40-b244-443d-ea2d-ef7fb05b526a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 78.5M  100 78.5M    0     0  4987k      0  0:00:16  0:00:16 --:--:-- 6031k\n"
     ]
    }
   ],
   "source": [
    "# The local location to store the reddit dataset.\n",
    "local_file = \"coarse_discourse_dump_reddit.json\"\n",
    "\n",
    "# The ! performs a shell command to download the reddit dataset using curl (not on Windows).\n",
    "!curl -o  $local_file https://storage.googleapis.com/tad2018/coarse_discourse_dump_reddit.json\n",
    "\n",
    "#!gsutil cp gs://tad2018/coarse_discourse_dump_reddit.json /tmp/coarse_discourse_dump_reddit.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPyo3mtPvqux"
   },
   "source": [
    "Let's extract the post content out into a posts global data frame that we'll use for our processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "a-XGj_AFCMBz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110595\n"
     ]
    }
   ],
   "source": [
    "# The reddit thread structure is nested with posts in a new content.\n",
    "# This block reads the file as json and creates a posts data frame.\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# A temporary variable to store the list of posts.\n",
    "posts_tmp = list()\n",
    "\n",
    "with open(local_file) as jsonfile:\n",
    "  for i, line in enumerate(jsonfile):\n",
    "    thread = json.loads(line)\n",
    "    for post in thread['posts']:\n",
    "      # Keep the thread title, subreddit, and url with each post.\n",
    "      posts_tmp.append((thread['subreddit'], thread['title'], thread['url'],\n",
    "                        post['id'], post.get('author', \"\"), post.get('body', \"\")))\n",
    "print(len(posts_tmp))\n",
    "\n",
    "# Create the posts data frame with the right column labels.  \n",
    "labels = ['subreddit', 'title', 'id', 'url', 'author', 'body']\n",
    "post_frame = pd.DataFrame(posts_tmp, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vtsmI785z32"
   },
   "source": [
    "## Tokenization and stemming with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pdb8u7RT0rDj"
   },
   "source": [
    "Let's perform some basic tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Mkk-DAI8O1l3"
   },
   "outputs": [],
   "source": [
    "# NLTK is the Natural Language Toolkit, and contains several language datasets\n",
    "# as well as implementations of many popular NLP algorithms.\n",
    "# HINT: You should look at what is available here when thinking about your coursework!\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# A simple tokenizer based on a regular expression; a series of whitespace chars.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_9WrpED3FDQ"
   },
   "source": [
    "We first create a basic 'normalization' or 'canonicalization' function that converts text into a standard format. The simple function performs tokenization and lowercasing. It uses the NLTK regular expression tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eRZLyxV23D7V"
   },
   "outputs": [],
   "source": [
    "def basic_canonicalize(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = tokenizer.tokenize(string)\n",
    "  for t in tokens:\n",
    "    # Normalizes the token by lowercasing it.\n",
    "    normalized_tokens.append(t.lower())\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLrpONOaDb6M"
   },
   "source": [
    "Now that we have our tokenizer, let's use it to start exploring the collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Vo-Ff6UHDdVM"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# This tokenizes the body posts and creates vector of tokens for each post.\n",
    "all_posts_tokenized = post_frame.body.apply(basic_canonicalize)\n",
    "\n",
    "# A single variable to hold the tokens across all posts.\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "\n",
    "# Put the entire array of tokens into a NLTK Frequency Distribution class.\n",
    "word_dist = nltk.FreqDist(all_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFW1L_BKHW_B"
   },
   "source": [
    "**Exercise**: \n",
    "\n",
    "*   How many posts are empty?\n",
    "*   What is the total number of words?\n",
    "*   What is the average length of the post?\n",
    "\n",
    "\n",
    "Use the API of NLTK's [FreqDist](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist) class to calculate the total number of words, the average length of a post. Using FreqDist for this may be overkill here, but it's a useful class for modeling language that we'll see next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fgxsuewdJNxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "for post in all_posts_tokenized:\n",
    "    if len(post)==0:\n",
    "        #print(post)\n",
    "        counter=counter+1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YCi7dM3FwfV"
   },
   "source": [
    "**Exercise**: Print out the 50 most frequent words in the collection, and their frequency using the FreqDist API. Hint: FreqDist extends [collections.Counter](https://docs.python.org/2/library/collections.html#collections.Counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WaMgJL4XJVZD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 176166),\n",
       " ('i', 146882),\n",
       " ('to', 124121),\n",
       " ('a', 113949),\n",
       " ('and', 102499),\n",
       " ('it', 83866),\n",
       " ('you', 77818),\n",
       " ('of', 73506),\n",
       " ('that', 61859),\n",
       " ('is', 60014),\n",
       " ('in', 56321),\n",
       " ('for', 48310),\n",
       " ('s', 40905),\n",
       " ('t', 39430),\n",
       " ('on', 34514),\n",
       " ('but', 34365),\n",
       " ('with', 33206),\n",
       " ('have', 33180),\n",
       " ('be', 32589),\n",
       " ('this', 29944),\n",
       " ('my', 29500),\n",
       " ('if', 28520),\n",
       " ('are', 25468),\n",
       " ('not', 25461),\n",
       " ('as', 25383),\n",
       " ('can', 23862),\n",
       " ('or', 23414),\n",
       " ('was', 23357),\n",
       " ('so', 22401),\n",
       " ('they', 22291),\n",
       " ('just', 21495),\n",
       " ('your', 20866),\n",
       " ('like', 20160),\n",
       " ('at', 18739),\n",
       " ('what', 16815),\n",
       " ('do', 16595),\n",
       " ('would', 16367),\n",
       " ('there', 16150),\n",
       " ('m', 16050),\n",
       " ('me', 15910),\n",
       " ('get', 15712),\n",
       " ('all', 15610),\n",
       " ('he', 15354),\n",
       " ('from', 14850),\n",
       " ('one', 14605),\n",
       " ('out', 14295),\n",
       " ('about', 14281),\n",
       " ('will', 14073),\n",
       " ('up', 14037),\n",
       " ('don', 13540)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InSKMVLs3Skl"
   },
   "source": [
    "What do you see?  The most common words are functional words, often referred to as 'stopwords' because they contain little information.  We should clean the data by removing these 'noise' words that don't have significant meaning on their own.\n",
    "\n",
    "We need a 'real' tokenizer that does stopping, stemming, and filters out other 'noise' words. It just so happens that NLTK has a built in stopword list. Download it and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hOs4QgaeexI4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/motoofi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFx5O4wQA3oT"
   },
   "source": [
    "**Exercise:** Create a function, `canonicalize` that filters 'meaningless' words.\n",
    "\n",
    "Modify the function to filter out 'noise' words. Use the NLTK Porter stemmer to stem the text.\n",
    "*   What about 'stop' words?\n",
    "*   What about excessively short tokens or 'long' tokens?\n",
    "\n",
    "Hint: Does the order matter? What could happen if stemming is applied after stopping?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean8.02716631884584\n",
      "std:7.798074686968009\n",
      "max:1440\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokens = word_dist.keys()\n",
    "lengths = []\n",
    "for token in tokens:\n",
    "    lengths.append(len(token))\n",
    "std = np.std(lengths)\n",
    "mean = np.mean(lengths)\n",
    "print(\"mean{0}\".format(mean))\n",
    "print(\"std:{0}\".format(std))\n",
    "print(\"max:{0}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F08gmxJQwtdL"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the NLTK Porter Stemmer discussed in lecture to use.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Modify the code below to 1) not append noisy words: words in stop_words or words that are too long or too short.\n",
    "# Question: What are good values for too long or too short? Why?\n",
    "def canonicalize(string):\n",
    "  normalized_tokens = list()\n",
    "  tokens = tokenizer.tokenize(string)\n",
    "  for token in tokens:\n",
    "    # YOUR CODE HERE\n",
    "    # Ignore stop words and other 'noise' words (too short? too long?)\n",
    "    # Lowercase and stem the words.\n",
    "    # Does the order of stopping and stemming matter? \n",
    "    #if (token in stop_words) or ((mean-std)<len(token)<(mean+std)) :\n",
    "    normalized = token.lower()\n",
    "    stem_and_normilized = stemmer.stem(normalized)\n",
    "    if (stem_and_normilized in stop_words):\n",
    "        pass\n",
    "    elif (len(stem_and_normilized)<(mean+std)) and (mean-std<len(stem_and_normilized)):\n",
    "        normalized_tokens.append(stem_and_normilized)\n",
    "  return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vl2IuLNcHpi-"
   },
   "source": [
    "Now, let's repeat our exercise.  How have the statistics changed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h_XrKqR6N-8g"
   },
   "outputs": [],
   "source": [
    "# This tokenizes the body posts and creates vector of tokens for each post.\n",
    "all_posts_tokenized = post_frame.body.apply(canonicalize)\n",
    "\n",
    "# A single variable to hold the tokens across all posts.\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "\n",
    "# Put the entire array of tokens into a NLTK Frequency Distribution class.\n",
    "word_dist = nltk.FreqDist(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYJB76cDK5iK"
   },
   "source": [
    "Use your code for most frequent words discussed above here to print out the most frequent 50 words again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZUUH6MNGLdbT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 29957),\n",
       " ('wa', 23377),\n",
       " ('like', 22210),\n",
       " ('get', 20548),\n",
       " ('would', 16367),\n",
       " ('one', 15873),\n",
       " ('http', 15796),\n",
       " ('go', 14118),\n",
       " ('use', 13954),\n",
       " ('com', 13199),\n",
       " ('time', 12666),\n",
       " ('think', 12474),\n",
       " ('make', 11450),\n",
       " ('know', 10671),\n",
       " ('realli', 10655),\n",
       " ('game', 10593),\n",
       " ('want', 10370),\n",
       " ('good', 10328),\n",
       " ('ha', 9977),\n",
       " ('peopl', 9729),\n",
       " ('also', 9548),\n",
       " ('work', 9278),\n",
       " ('look', 9177),\n",
       " ('becaus', 8956),\n",
       " ('thing', 8920),\n",
       " ('onli', 8770),\n",
       " ('ani', 8708),\n",
       " ('tri', 8512),\n",
       " ('much', 8295),\n",
       " ('need', 8157),\n",
       " ('play', 7873),\n",
       " ('see', 7862),\n",
       " ('could', 7492),\n",
       " ('way', 7429),\n",
       " ('2', 7231),\n",
       " ('hi', 7191),\n",
       " ('even', 7170),\n",
       " ('well', 7140),\n",
       " ('1', 7097),\n",
       " ('thank', 6904),\n",
       " ('www', 6889),\n",
       " ('say', 6813),\n",
       " ('take', 6544),\n",
       " ('veri', 6269),\n",
       " ('still', 6142),\n",
       " ('lot', 6126),\n",
       " ('someth', 6019),\n",
       " ('year', 6014),\n",
       " ('3', 5919),\n",
       " ('start', 5785)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmJapSu9IbUO"
   },
   "source": [
    "Looking at this list, it still contains reddit-specific words that may not be meaningful. For example, 'http'? Update the stop word set and repeat the above block until you are happy with the list.\n",
    "\n",
    "**Exercise: ** Modify the stop_words set to include additional noise words. Re-run the posts tokenization and top word calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "P0B-Ag-_y9MS",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 29957),\n",
       " ('wa', 23377),\n",
       " ('like', 22210),\n",
       " ('get', 20548),\n",
       " ('would', 16367),\n",
       " ('one', 15873),\n",
       " ('go', 14118),\n",
       " ('use', 13954),\n",
       " ('com', 13199),\n",
       " ('time', 12666),\n",
       " ('think', 12474),\n",
       " ('make', 11450),\n",
       " ('know', 10671),\n",
       " ('realli', 10655),\n",
       " ('game', 10593),\n",
       " ('want', 10370),\n",
       " ('good', 10328),\n",
       " ('ha', 9977),\n",
       " ('peopl', 9729),\n",
       " ('also', 9548),\n",
       " ('work', 9278),\n",
       " ('look', 9177),\n",
       " ('becaus', 8956),\n",
       " ('thing', 8920),\n",
       " ('onli', 8770),\n",
       " ('ani', 8708),\n",
       " ('tri', 8512),\n",
       " ('much', 8295),\n",
       " ('need', 8157),\n",
       " ('play', 7873),\n",
       " ('see', 7862),\n",
       " ('could', 7492),\n",
       " ('way', 7429),\n",
       " ('2', 7231),\n",
       " ('hi', 7191),\n",
       " ('even', 7170),\n",
       " ('well', 7140),\n",
       " ('1', 7097),\n",
       " ('thank', 6904),\n",
       " ('www', 6889),\n",
       " ('say', 6813),\n",
       " ('take', 6544),\n",
       " ('veri', 6269),\n",
       " ('still', 6142),\n",
       " ('lot', 6126),\n",
       " ('someth', 6019),\n",
       " ('year', 6014),\n",
       " ('3', 5919),\n",
       " ('start', 5785),\n",
       " ('first', 5750)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.update(('http', ))\n",
    "all_posts_tokenized = post_frame.body.apply(canonicalize)\n",
    "all_tokens = list(itertools.chain.from_iterable(all_posts_tokenized))\n",
    "word_dist = nltk.FreqDist(all_tokens)\n",
    "word_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JBfGbpYBO8N"
   },
   "source": [
    "We will now look at how we represent words using a one-hot encoding in more detail.\n",
    "\n",
    "Look at the Vocabulary class below:\n",
    "- What does it do? \n",
    "- How are words mapped to numbers? \n",
    "- How are the words ordered? Why might we want to do this?\n",
    "- What is the 'unk' token? And when is it used?\n",
    "- What happens if you perform words_to_ids on a word not in the original tokens?\n",
    "- Why might you want special start and end tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O_F9De9u2YZ9"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  START_TOKEN = \"<s>\"\n",
    "  END_TOKEN = \"</s>\"\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    # Counter is a very useful built-in python collection for keeping counts, \n",
    "    # Instead of extending Counter like FreqDist, it's used as a member variable.\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    self.num_unigrams = sum(iter(self.unigram_counts.values()))\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
    "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency.\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in iter(self.id_to_word.items())}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience keep a set of unique words.\n",
    "    self.wordset = set(iter(self.word_to_id.keys()))\n",
    "\n",
    "    # Store special IDs.\n",
    "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def sentence_to_ids(self, words):\n",
    "    return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM2JtTPoI0O_"
   },
   "source": [
    "**Exercise:** Use the Vocabulary class to print out the top 10 most frequent unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kRAQnbqiMpe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', 'thi', 'wa', 'like', 'get', 'would', 'one']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary(all_tokens)\n",
    "x=vocab.ordered_words()\n",
    "x[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KQ3qqZEMqii"
   },
   "source": [
    "Below are some example uses of the Vocabulary object to map words to ids and vice-versa. Execute the code and make sure you understand what is happening with the Vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "A1wNxrOEMrNC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[2]\n",
      "['<unk>']\n",
      "16120 ['hummu']\n"
     ]
    }
   ],
   "source": [
    "# Use our vocabulary to map words to integer ids.\n",
    "# You should be able to predict the index of like based on your knowledge\n",
    "# the Vocabulary class and the most frequent words.\n",
    "print(vocab.words_to_ids([\"like\"]))\n",
    "\n",
    "# What index should this return?\n",
    "print(vocab.words_to_ids([\"likemymadeupword\"]))\n",
    "\n",
    "\n",
    "# What's special about 2? Can you predict what it will be?\n",
    "print(vocab.ids_to_words([2]))\n",
    "\n",
    "\n",
    "# And let's print out the words from the vocab.\n",
    "import random as rand\n",
    "\n",
    "# Pick a random token in the vocabulary\n",
    "idx = rand.randint(0, vocab.size-1)\n",
    "print(idx, vocab.ids_to_words([idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XWjf3yuTO6J"
   },
   "source": [
    "## Vector representations with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25XYnTCi2CWz"
   },
   "source": [
    "There are lots of ways to do the same thing in Python. Scikit-learn is a widely machine learning library that includes tools for performing operations on data: similarity computation, clustering, classification, and many others. We'll use Scikit-learn to create a vector representations of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q997lE7FTiyo"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-78971ac4a0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Parallel arrays of the post keys and values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpost_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpost_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Parallel arrays of the post keys and values.\n",
    "post_vals = list()\n",
    "post_keys = list()\n",
    "\n",
    "for index, post in post_frame.iterrows():\n",
    "    post_keys.append(post['id'])\n",
    "    post_vals.append(post['body'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qum0RqrykCDF"
   },
   "source": [
    "Create a simple TF term-document matrix with the TF counts using the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ETqhytXLkBDH"
   },
   "outputs": [],
   "source": [
    "# We pass in our function tokenizer to the vectorizer object.\n",
    "tf_vectorizer = CountVectorizer(tokenizer=canonicalize)\n",
    "\n",
    "# This creates a vocabulary like our Vocabulary object above.\n",
    "tf_vectorizer.fit(post_vals)\n",
    "\n",
    "# Now we create a sparse term-document matrix using the vocabulary.\n",
    "# This performs the mapping of tokens to their IDs.\n",
    "tf_term_document_matrix = tf_vectorizer.transform(post_vals)\n",
    "\n",
    "# Note: These can be combined with fit_transform to do this in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqXwInGGOo2N"
   },
   "source": [
    "**Exercise**: Type in a string below. Look at it's vector representation. What happens to 'unk' words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4p0tn37ZW1Ji"
   },
   "outputs": [],
   "source": [
    "str = '**YOUR STRING HERE**'\n",
    "response = tf_vectorizer.transform([str])\n",
    "print (response)\n",
    "print (tf_vectorizer.inverse_transform(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0Q0vgPe3dqD"
   },
   "source": [
    "The first step transforms our sentence using the vocbulary, creating a sparse TF vector representing the sentence.\n",
    "The second operation allows us to see what words correspond to each of those elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXmMp1CTWaWi"
   },
   "source": [
    "**Exercise:** Let's now upgrade from simple counts to a vectorizer that uses IDF. See the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) API.  The TF-IDF Vectorizer uses the raw term frequency by default, use the version of the constructor that uses the log(tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wh6T4S4mUcXR"
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from sklearn and fill in the code below:\n",
    "tfidf_vectorizer = ...\n",
    "tfidf_term_document_matrix = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1n8F135ZI6JO"
   },
   "source": [
    "TF-IDF vectorizer also includes support for creating n-grams during tokenization. Check the documentation for TfidfVectorizer and create an instance that includes word bigram tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jbr5xeEaI264"
   },
   "outputs": [],
   "source": [
    "# Limit the size of the vocabulary to the N most common words.\n",
    "num_features=500000\n",
    "\n",
    "ngram_vectorizer = ...\n",
    "ngram_term_document_matrix = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNJNCL46P3iI"
   },
   "source": [
    "Compare the n-gram representation of your string using the ngram vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XdGeP28VXdA3"
   },
   "outputs": [],
   "source": [
    "ngram_matrix = ngram_vectorizer.transform([str])\n",
    "print (ngram_matrix)\n",
    "print (ngram_vectorizer.inverse_transform(ngram_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5QjuYZn4pyx"
   },
   "source": [
    "We're just scratching the surface of what's possible with SKLearn's vectorizers.  There are other types of representations as well. For example, there is the [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) discussed in lecture as well others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEplJqQ4Zl4l"
   },
   "source": [
    "## Cosine similarity\n",
    "We will now use sklearn's cosine similarity implementation to find the most similar posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rWR6NZZIJKe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    " \n",
    "# A function that given an input query item returns the top-k most similar items \n",
    "# by their cosine similarity.\n",
    "def find_similar(query_vector, td_matrix, top_k = 5):\n",
    "    cosine_similarities = cosine_similarity(query_vector, td_matrix).flatten()\n",
    "    sorted_indices = cosine_similarities.argsort()[::-1]\n",
    "    return [(index, cosine_similarities[index]) for index in sorted_indices][0:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zc6zTizJLgs"
   },
   "source": [
    "**Exercise:** Next, lets pick a post to compute similarity with. Put the content of a post in the variable called `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PwE6Gu6YJXm0"
   },
   "outputs": [],
   "source": [
    "# Create an input 'query' string to find a similar post.\n",
    "query = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "smf4clmop-KY"
   },
   "outputs": [],
   "source": [
    "# Temporary variables to easily change the vectorizer and the document_matrix.\n",
    "vectorizer = tfidf_vectorizer\n",
    "document_matrix = tfidf_term_document_matrix\n",
    "\n",
    "\n",
    "# Below are options to use an existing post content as a string query.\n",
    "#import random as rand\n",
    "#post_index = 1000\n",
    "#post_index = rand.randint(0, len(post_vals))\n",
    "#str = post_vals[post_index]\n",
    "print(str.replace('\\n', ''))\n",
    "\n",
    "# Transform our string using the vocabulary.\n",
    "transformed = vectorizer.transform([query])\n",
    "query_transformed = transformed[0:1]\n",
    "print(query_transformed)\n",
    "\n",
    "top_k = 10\n",
    "print (\"\\nSimilar:\")\n",
    "for index, score in find_similar(query_transformed, document_matrix, top_k):\n",
    "  print(score, index, post_keys[index], post_vals[index].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W66BwN2Z7dPc"
   },
   "source": [
    "What do you see?  If you use the post, it should find itself and return a similarity score of 1.0.\n",
    "\n",
    "Try changing the vectorizers and matrix representations to the ones you constructed (count, tfidf, or ngrams). \n",
    "How do the most similar posts change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIx1ORo4Ziga"
   },
   "source": [
    "## KMeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUhTG7OQ8Wag"
   },
   "source": [
    "What's in the reddit dataset? When we want to explore a dataset, one way to that is to cluster the data.\n",
    "\n",
    "As discussed in lecture, we'll now use an unsupervised clustering algorithm to cluster our posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dj6xJlnz8BAN"
   },
   "source": [
    "\n",
    "From the SKlearn documentation:\n",
    "*The [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.*\n",
    "\n",
    "*K-means is often referred to as Lloyd’s algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose k samples from the dataset X. After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yBFqE3-Mb2eE"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 20\n",
    "\n",
    "# Note that there are multiple implementation of KMeans in sklearn. \n",
    "# The minibatchkmeans is faster, but less stable than the original kmeans.\n",
    "# It's faster, but produces different clusterings.\n",
    "#kmeans = MiniBatchKMeans(n_clusters=num_clusters, batch_size=500,  max_no_improvement=3)\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, n_init=2, verbose=10)\n",
    "kmeans.fit(document_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obt13q2h85su"
   },
   "source": [
    "We should now have a kmeans clustering with cluster centers, called 'centroids'. We'll print out the top 10 terms from each of the centroids. This will help us understand what each cluster represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GMZQiQtddm4s"
   },
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "tokens = vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "  print(\"Cluster %d:\" % i)\n",
    "  for ind in centroids[i, :10]:\n",
    "    print(' %s' % tokens[ind])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8f0AE1Y9_-G"
   },
   "source": [
    "Could you put a label on the clusters? Do the cluster terms look meaningful?\n",
    "\n",
    "\n",
    "Let's explore the clustering in more detail by looking at the assignments of posts to the clusters. We'll print out the contents by looking a sample of the top 10 posts per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aTkj56Tzklk3"
   },
   "outputs": [],
   "source": [
    "# Group the posts by their cluster labels.\n",
    "clustering = collections.defaultdict(list)\n",
    "for idx, label in enumerate(kmeans.labels_):\n",
    "  clustering[label].append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rcwzuz7vlvS5"
   },
   "outputs": [],
   "source": [
    "for cluster, indices in clustering.items():\n",
    "  print(\"\\nCluster:\", cluster, \" Num posts: \", len(indices))\n",
    "  cur_docs = 0\n",
    "  for index in indices:\n",
    "    if (cur_docs > 10):\n",
    "      break\n",
    "    post_contents = post_vals[index].replace('\\n', '')\n",
    "    print(index, post_keys[index], (post_contents[:75] + '..') if len(post_contents) > 75 else post_contents)\n",
    "    cur_docs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKLaVZHMAF52"
   },
   "source": [
    "\n",
    "\n",
    "*   Is the clustering useful to explore the data?\n",
    "*   Can you label the clusters now that you've seen some examples?\n",
    "*   Are these 'good' clusters?\n",
    "\n",
    "If you have time, try changing the number of clusters - e.g 50 or 100. What do you observe about differences?\n",
    "\n",
    "Note: KMeans can be somewhat slow to execute exactly. See the MiniBatchKmeans variable above.  If you have time, try clustering using this clustering algorithm.  Do you see any significant differences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rW03I8Kn0KCq"
   },
   "source": [
    "## Summary \n",
    "\n",
    "\n",
    "*   We've explored one-hot vector representations of text using several libraries.\n",
    "*   We've created count, tf-idf, and n-gram representations of text vectors.\n",
    "*   We've used the vectors to perform unsupervised clustering using scikit learn.\n",
    "*   Clustered the posts using KMeans\n",
    "\n",
    "\n",
    "When and why is clustering useful?\n",
    "*   Clusters can be used as features in a task (text classification and others)\n",
    "*   Used to perform exploratory data analysis (What's in there?)\n",
    "*   A fast, easy baseline that scales well and doesn't required labeled data\n",
    "\n",
    "Next time we'll experiment with additional language prediction tasks using the Vocabulary data structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riYInv1ZShyU"
   },
   "source": [
    "## Please complete the Moodle feedback quiz for this lab\n",
    "\n",
    "[Quiz for Lab 2](http://moodle2.gla.ac.uk/mod/feedback/view.php?id=827639)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1miNfZyHmBkI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Lab 2.ipynb",
   "provenance": [
    {
     "file_id": "1zKMToiljPWRD8lp69HATAhkRj-OqefOV",
     "timestamp": 1515772404172
    },
    {
     "file_id": "1hNjXtYEZMlzFe3yJXN-4orZdmfX7QW9B",
     "timestamp": 1515088413611
    },
    {
     "file_id": "1Y80cJDxLkDZtvTyRcbPy8xMPAbyNmVXE",
     "timestamp": 1513709680152
    },
    {
     "file_id": "1fARiTtWW0pN80eg7XqEhbUbEtelPf5HT",
     "timestamp": 1513184264238
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
